<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Personal website of Lorenzo Baraldi, PhD sudent at Unimore.">
	<meta name="author" content="Lorenzo Baraldi">
    <title>Lorenzo Baraldi - PhD Student</title>
    <link href="./bootstrap/css/bootstrap.min.css" rel="stylesheet">
	<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="./bootstrap/academicicons/academicons.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="style.css" />
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-39563464-1', 'auto');
	  ga('require', 'displayfeatures');
      ga('send', 'pageview');
    </script>
  </head>
  <body>
  <div class="header">
  
      <div class="container">
        <div class="row">
          <div class="col-md-5">
            <h1>Lorenzo Baraldi</h1>
          </div>
        </div>
        <div class="row spacer">
          <div class="col-md-2">
            <img src="images/pic.JPG" class="img-rounded" width="150"/>
          </div>
          <div class="col-md-9">
            PhD Student @ <a href="http://imagelab.ing.unimore.it">Imagelab</a><br />
            <a href="http://www.ing.unimore.it">Engineering Department</a><br />
            <a href="http://www.unimore.it">University of Modena and Reggio Emilia</a> <br /> <br />
            <ul class="list-unstyled">
              <li>Email: <a href="mailto:lorenzo.baraldi@unimore.it">lorenzo.baraldi@unimore.it</a></li>
              <li>Curriculum: <a href="curriculum.pdf">C.V.</a></li>
			  <li><a href="https://scholar.google.it/citations?user=V4RuMvsAAAAJ&hl=en" class="icon"><i class="ai ai-google-scholar fa-4x"></i></a>&nbsp;<a href="https://twitter.com/lorenzo_baraldi" class="icon"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;<a href="http://it.linkedin.com/pub/lorenzo-baraldi/83/164/8b" class="icon"><i class="fa fa-linkedin fa-lg"></i></a></li>
            </ul>
          </div>
        </div>
      </div>

  </div>
  <div class="container">
  
	
  <div class="row">
	<div class="col-md-12">
	  <h2>News</h2>
	  <ul>
		
			<li>Paper accepted at ICMR 2016!</li>
		
			<li>We have received two hardware grants: the NVIDIA Hardware Grant and the Italian Supercomputing Resource Allocation (ISCRA) Grant from CINECA.</li>
		
	  </ul>
	</div>
  </div>	
 
	
  <div class="row">
	<div class="col-md-12">
	  <h2>Publications</h2>
		
			
			  <div class="row">
				<div class="col-md-12">
				  <h4>Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic Deep Features</h4>
				  <h5>L. Baraldi, C. Grana, R. Cucchiara</h5>
				  <h5>ICMR 2016</h5>
				  
					<img src="papers/2016_ICMR.png" />
				  
				  <p>This paper presents a novel retrieval pipeline for video collections, which aims to retrieve the most significant parts of an edited video for a given query, and represent them with thumbnails which are at the same time semantically meaningful and aesthetically remarkable. Videos are first segmented into coherent and story-telling scenes, then a retrieval algorithm based on deep learning is proposed to retrieve the most significant scenes for a textual query. A ranking strategy based on deep features is finally used to tackle the problem of visualizing the best thumbnail. Qualitative and quantitative experiments are conducted on a collection of edited videos to demonstrate the effectiveness of our approach.</p>
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2016_ICMR.pdf"><span class="btn btn-primary btn-sm">PDF</span></a>
					
				  
				</div>
			  </div>
			  <hr>	  
			
			
			
		
			
			  <div class="row">
				<div class="col-md-12">
				  <h4>A Deep Siamese Network for Scene Detection in Broadcast Videos</h4>
				  <h5>L. Baraldi, C. Grana, R. Cucchiara</h5>
				  <h5>ACM Multimedia 2015</h5>
				  
					<img src="papers/2015ACMM.png" />
				  
				  <p>We present a model for scene detection that learns a distance measure between shots. We go beyond traditional hand-crafted features and apply the deep learning paradigm, exploiting both visual and textual features from the transcript. Deeply learned features are then used together with a clustering algorithm to segment the video. To the best of our knowledge, this is the first attempt to use deep learning in this task. We also propose and release a new benchmark dataset.</p>
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015ACMM_Scenes.pdf"><span class="btn btn-primary btn-sm">PDF</span></a>
					
						&nbsp;
					
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/page.asp?IdPage=5"><span class="btn btn-primary btn-sm">Dataset</span></a>
					
						&nbsp;
					
				  
					<a href="https://gist.github.com/baraldilorenzo/05b742d47220b487c8bd"><span class="btn btn-primary btn-sm">Caffe models</span></a>
					
				  
				</div>
			  </div>
			  <hr>	  
			
			
			
		
			
			  <div class="row">
				<div class="col-md-12">
				  <h4>Scene segmentation using Temporal Clustering for Accessing and Re-using Broadcast Video</h4>
				  <h5>L. Baraldi, C. Grana, R. Cucchiara</h5>
				  <h5>ICME 2015</h5>
				  
					<img src="papers/2015ICME.png" />
				  
				  <p>Scene detection is a fundamental tool for allowing effective video browsing and re-using. In this paper we present a model that automatically divides videos into coherent scenes, which is based on a novel combination of local image descriptors and temporal clustering techniques. Experiments are performed to demonstrate the effectiveness of our approach, by comparing our algorithm against two recent proposals for automatic scene segmentation.</p>
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015ICME.pdf"><span class="btn btn-primary btn-sm">PDF</span></a>
					
						&nbsp;
					
				  
					<a href="http://imagelab.ing.unimore.it/files/RaiSceneDetection.zip"><span class="btn btn-primary btn-sm">Dataset</span></a>
					
				  
				</div>
			  </div>
			  <hr>	  
			
			
			
		
			
			  <div class="row">
				<div class="col-md-12">
				  <h4>Measuring Scene Detection Performance</h4>
				  <h5>L. Baraldi, C. Grana, R. Cucchiara</h5>
				  <h5>IbPRIA 2015</h5>
				  
					<img src="papers/2015IbPRIA.png" />
				  
				  <p>In this paper we evaluate the performance of scene detection techniques, starting from the classic precision/recall approach, moving to the better designed coverage/overflow measures, and finally proposing an improved metric, in order to solve frequently observed cases in which the numeric interpretation is different from the expected results. Numerical evaluation is performed on two recent proposals for automatic scene detection, and comparing them with a simple but effective novel approach. Experimental results are conducted to show how different measures may lead to different interpretations.</p>
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015IbPRIA.pdf"><span class="btn btn-primary btn-sm">PDF</span></a>
					
						&nbsp;
					
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015IRCDL.pdf"><span class="btn btn-primary btn-sm">Dataset</span></a>
					
				  
				</div>
			  </div>
			  <hr>	  
			
			
			
		
			
			
			
				<div class="row">
					<div class="col-md-9">
					  <h5>Layout analysis and content classification in digitized books, IRCDL 2016
					  
						(<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2016_IRCDL.pdf">PDF</a>)
					  
					  </h5>
					</div>
				</div>
						
			
		
			
			
			
				<div class="row">
					<div class="col-md-9">
					  <h5>Shot, scene and keyframe ordering for interactive video re-use, VISAPP 2015
					  
						(<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2016_VISAPP.pdf">PDF</a>&nbsp;<a href="http://imagelab.ing.unimore.it/files/RaiSceneDetection.zip">Dataset</a>)
					  
					  </h5>
					</div>
				</div>
						
			
		
			
			
			
				<div class="row">
					<div class="col-md-9">
					  <h5>Shot and scene detection via hierarchical clustering for re-using broadcast video, CAIP 2015
					  
						(<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015CAIP.pdf">PDF</a>&nbsp;<a href="http://imagelab.ing.unimore.it/files/RaiSceneDetection.zip">Dataset</a>&nbsp;<a href="http://imagelab.ing.unimore.it/files/ShotDetector.zip">Source code</a>)
					  
					  </h5>
					</div>
				</div>
						
			
		
			
			
			
				<div class="row">
					<div class="col-md-9">
					  <h5>Analysis and Re-use of Videos in Educational Digital Libraries with Automatic Scene Detection, IRCDL 2015
					  
						(<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015IRCDL.pdf">PDF</a>&nbsp;<a href="http://imagelab.ing.unimore.it/files/RaiSceneDetection.zip">Dataset</a>)
					  
					  </h5>
					</div>
				</div>
				<hr />		
			
		
			
			  <div class="row">
				<div class="col-md-12">
				  <h4>Gesture Recognition using Wearable Vision Sensors to Enhance Visitors' Museum Experiences</h4>
				  <h5>L. Baraldi, F. Paci, G. Serra, L. Benini, R. Cucchiara</h5>
				  <h5>IEEE Sensors Journal</h5>
				  
					<img src="papers/2015IEEESens.png" />
				  
				  <p>We introduce a novel approach to cultural heritage experience: by means of ego-vision embedded devices we develop a system which offers a more natural and entertaining way of accessing museum knowledge. Our method is based on distributed self-gesture and artwork recognition, and does not need fixed cameras nor RFIDs sensors. We propose the use of dense trajectories sampled around the hand region to perform selfgesture recognition, understanding the way a user naturally interacts with an artwork, and demonstrate that our approach can benefit from distributed training. We test our algorithms on publicly available datasets and we extend our experiments to both virtual and real museum scenarios where our method shows robustness when challenged with real-world data. Furthermore, we run an extensive performance analysis on our ARM-based wearable device.</p>
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2015IEEESens.pdf"><span class="btn btn-primary btn-sm">PDF</span></a>
					
						&nbsp;
					
				  
					<a href="http://imagelab.ing.unimore.it/files/ego_virtualmuseum.zip"><span class="btn btn-primary btn-sm">Dataset</span></a>
					
				  
				</div>
			  </div>
			  <hr>	  
			
			
			
		
			
			  <div class="row">
				<div class="col-md-12">
				  <h4>Gesture Recognition in Ego-Centric Videos using Dense Trajectories and Hand Segmentation</h4>
				  <h5>L. Baraldi, F. Paci, G. Serra, L. Benini, R. Cucchiara</h5>
				  <h5>CVPR Workshop 2014</h5>
				  
					<img src="papers/2014EVW.png" />
				  
				  <p>We present a novel method for monocular hand gesture recognition in ego-vision scenarios that deals with static and dynamic gestures and can achieve high accuracy results using a few positive samples. Specifically, we use and extend the dense trajectories approach that has been successfully  introduced for action recognition. Dense features are extracted around regions selected by a new hand segmentation technique that integrates superpixel classification, temporal and spatial coherence. We extensively test our gesture recognition and segmentation algorithms on public datasets and propose a new dataset shot with a wearable camera. In addition, we demonstrate that our solution can work in near real-time on a wearable device.</p>
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2014EVW.pdf"><span class="btn btn-primary btn-sm">PDF</span></a>
					
						&nbsp;
					
				  
					<a href="http://imagelab.ing.unimore.it/files/ego_virtualmuseum.zip"><span class="btn btn-primary btn-sm">Dataset</span></a>
					
				  
				</div>
			  </div>
			  <hr>	  
			
			
			
		
			
			  <div class="row">
				<div class="col-md-12">
				  <h4>Hand Segmentation for Gesture Recognition in EGO-Vision</h4>
				  <h5>G. Serra, M. Camurri, L. Baraldi, M. Benedetti, R. Cucchiara</h5>
				  <h5>IMMPD 2013</h5>
				  
					<img src="papers/2013IMMPD.png" />
				  
				  <p>Portable devices for first-person camera views will play a central role in future interactive systems. One necessary step for feasible human-computer guided activities is gesture recognition, preceded by a reliable hand segmentation from egocentric vision. In this work we provide a novel hand segmentation algorithm based on Random Forest superpixel classification that integrates light, time and space consistency. We also propose a gesture recognition method based Exemplar SVMs since it requires a only small set of positive sampels, hence it is well suitable for the egocentric video applications. Furthermore, this method is enhanced by using segmented images instead of full frames during test phase. Experimental results show that our hand segmentation algorithm outperforms the state-of-the-art approaches and improves the gesture recognition accuracy on both the publicly available EDSH dataset and our dataset designed for cultural heritage applications.</p>
				  
					<a href="http://imagelab.ing.unimore.it/imagelab/pubblicazioni/2013IMMPD.pdf"><span class="btn btn-primary btn-sm">PDF</span></a>
					
				  
				</div>
			  </div>
			  <hr>	  
			
			
			
		
	</div>
  </div>	
 
	<div class="row">
<div class="col-md-12">
  <h2>Teaching</h2>
</div>
</div>

<div class="row">
<div class="col-md-12">
  <dl class="dl-horizontal">
	<dt>Laboratory Lecturer</dt>
	<dd>Computer Vision (2016)</dd>
	
  </dl>
</div>
</div>		

<div class="row">
<div class="col-md-12">
  <dl class="dl-horizontal">
	<dt>Tutor</dt>
	<dd>Fundamentals of Computer Science I (2015)</dd>
	
  </dl>
</div>
</div>		

<div class="row">
<div class="col-md-12">
  <dl class="dl-horizontal">
	<dt>Lecturer</dt>
	<dd>Pattern Recognition and Machine Learning (2015)</dd>
	
		<dd>Three lectures on supervised and unsupervised Deep Learning [<a href="https://github.com/baraldilorenzo/DL_tutorials">Lab notes</a>]</dd>
	
  </dl>
</div>
</div>		

<div class="row">
<div class="col-md-12">
  <dl class="dl-horizontal">
	<dt>Laboratory lecturer</dt>
	<dd>Graduate Master in Visual Computing and Multimedia Technologies (2015)</dd>
	
  </dl>
</div>
</div>		

<div class="row">
<div class="col-md-12">
  <dl class="dl-horizontal">
	<dt>Laboratory lecturer</dt>
	<dd>Computer Vision (2015)</dd>
	
  </dl>
</div>
</div>		

	  <div class="row">
	<div class="col-md-12">
	  <h2>Education</h2>
	</div>
  </div>
  <div class="row">
	<div class="col-md-12">
	  <dl class="dl-horizontal">
		<dt>Bachelor Degree</dt>
		<dd>B.Sc. in Computer Engineering, 2011<br />
		  University of Modena and Reggio Emilia<br />
		  Mark: 110/110 cum laude
		</dd>
	  </dl>
	</div>
  </div>
  <div class="row">
	<div class="col-md-12">
	  <dl class="dl-horizontal">
		<dt>Master Degree</dt>
		<dd>M.Sc. in Computer Engineering, 2014<br />
		  University of Modena and Reggio Emilia<br />
		  Mark: 110/110 cum laude
		</dd>
	  </dl>
	</div>
  </div>	  



  <footer>
	<div class="row">
	  <div class="col-md-12">
		<p>Last update: April 2016</p>
	  </div>
	</div>
  </footer>
</div>

<script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
<script src="./bootstrap/js/bootstrap.min.js"></script>
</body>
</html>